{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of automatically categorizing texts into some classes. \n",
    "\n",
    "Popular methods of text classification: Naive Bayes, Logistic Regression, Support Vector Machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier. Наивный Байесовский классификатор. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call it naive cause we assume that all features are independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, features):\n",
    "    lst = re.findall('\\w+', text.lower())\n",
    "    v = []\n",
    "    dct = {}\n",
    "    for w in lst:\n",
    "        if w not in dct:\n",
    "            dct[w] = 1\n",
    "        else:\n",
    "            dct[w] += 1\n",
    "    \n",
    "    for i in features:\n",
    "        #v.append(lst.count(i))    #сount очень дорогая операция,  т.е здесь сложность = квадрат\n",
    "        if i in dct:\n",
    "            v.append(dct[i])\n",
    "        else:\n",
    "            v.append(0)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text = (open('input.txt', encoding='UTF-8')).read()\n",
    "features = sorted(open('features.txt', encoding='UTF-8').read().split())\n",
    "print(extract_features(text, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nbc(features, corpora, classes):\n",
    "    if classes[0] != 'positive':\n",
    "        classes[0], classes[1] =  classes[1], classes[0]\n",
    "    lst=[[], []]\n",
    "    \n",
    "    for i,e in enumerate(corpora):\n",
    "        tokens = re.findall('\\w+', e.lower())\n",
    "        dct = {}\n",
    "        d = [t for t in tokens if t in features]\n",
    "        for w in d:\n",
    "            if w not in dct:\n",
    "                dct[w] = 1\n",
    "            else:\n",
    "                dct[w] += 1\n",
    "    \n",
    "        V = len(d)\n",
    "        #print(len(features))\n",
    "        #V = len(tokens)    #все слова, НО надо считать только кол-во фичей\n",
    "        V_extra = len(set(d))    #это нужно, чтобы частоты ФИЧЕЙ V_pos/V_neg были сопоставимы\n",
    "        for j in features:\n",
    "            if j in dct:\n",
    "                lst[i].append((dct[j] + 1)/(V + V_extra))\n",
    "                #lst[0].append((dct[j])/(V))\n",
    "            else:\n",
    "                lst[i].append(0)\n",
    "                \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text, features, classes, priors, Params):\n",
    "    p1 = 1.0\n",
    "    p2 = 1.0    \n",
    "    \n",
    "    v = extract_features(text, features)\n",
    "    for i in range(len(features)):\n",
    "        k = v[i]\n",
    "        \n",
    "        #if Params[0][i] != 0.0:\n",
    "        #    p1 *= (Params[0][i] ** k)\n",
    "        #if Params[1][i] != 0.0:\n",
    "        #    p2 *= (Params[1][i] ** k)\n",
    "        p1 *= (Params[0][i] ** k)\n",
    "        p2 *= (Params[1][i] ** k)\n",
    "    positive_probability = priors[0] * p1\n",
    "    negative_probability = priors[1] * p2\n",
    "    \n",
    "    print('positive_probability', positive_probability)\n",
    "    print('negative_probability', negative_probability)\n",
    "    \n",
    "    if positive_probability == negative_probability:\n",
    "        return 'equal'\n",
    "    elif positive_probability > negative_probability:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive_probability 8.235317441580478e-12\n",
      "negative_probability 4.171376137406484e-11\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "features = sorted(open('features.txt', encoding='UTF-8').read().split())\n",
    "\n",
    "corpus_pos = open('pos_train.txt', encoding='UTF-8').read()\n",
    "corpus_neg = open('neg_train.txt', encoding='UTF-8').read()\n",
    "corpora = corpus_pos, corpus_neg    #tuple\n",
    "\n",
    "classes = 'positive', 'negative'\n",
    "\n",
    "Params = train_nbc(features, corpora, classes)\n",
    "\n",
    "priors = 0.5, 0.5\n",
    "text = (open('input.txt', encoding='UTF-8')).read()\n",
    "\n",
    "label = classify(text, features, classes, priors, Params)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#feat = [...]    #O(n) сложность операции\n",
    "#feat = {...}    #O(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
